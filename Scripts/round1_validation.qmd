---
title: "Round 1 Simulation Report"
author: "July Pilowsky"
format: html
editor: visual
---

```{r setup}
library(tidyverse)
library(here)
library(paletteer)
library(furrr)
library(qs)
library(broom)
library(abind)
library(foreach)
library(doParallel)
i_am("Scripts/round1_validation.qmd")
# Register parallel backend with doParallel
num_cores <- parallel::detectCores() - 1  # Leave one core free
cl <- makeCluster(num_cores)
registerDoParallel(cl)
results_dir <- here("Data/Output/Round1_matrix") |> list.dirs() |> 
  gtools::mixedsort() |> _[-1]
round1_priors <- read_csv(here("Data/Input/sample_data_round1.csv")) |> 
  slice(results_dir |> str_sub(74) |> as.integer())
year_lookup <- data.frame(index = 1:77, Year = 1940:2016)
plan(multisession)
```

# Validation

Here I will score the simulations against various validation datasets and see how they fare.

## DC 1994

The first and most important validation is whether there are house finches in Washington DC in 1994. If this check is not passed, then the simulation terminates because the disease cannot be introduced there, which is when the disease did in fact start. This validation is the initial filter.

```{r dc}
# Are birds in DC?
dc <- results_dir |> map(list.files) |> map(length) |> map_lgl(\(x) x > 212)
# Graph
round1_priors |> 
  mutate(dc = dc) |> 
  ggplot(aes(x = "", y = dc, fill = dc)) +
  geom_col() +
  coord_polar(theta = "y") +
  theme_void() +
  scale_fill_paletteer_d("PrettyCols::Light", name = "Birds in DC?")
```

## Prevalence Trends

Here I compare trends in prevalence in the Northeast against observed patterns in Project FeederWatch.

```{r prevalence trends, eval = FALSE}
# Function to filter file paths based on the second number
filter_paths <- function(file_paths) {
  # Use str_extract to capture the last number before the underscore
  last_numbers <- as.numeric(str_extract(file_paths, "(?<=/)(\\d+)(?=_)"))
  
  # Filter paths where the last number is between 63 and 77
  filtered_paths <- file_paths[last_numbers >= 63 & last_numbers <= 77]
  
  return(filtered_paths)
}

# Validation data
mg_trends <- here("Data/Validation/prevalence_trends.csv") |> read_csv()

# Put file paths in the right order
file_paths <- results_dir[dc] |> 
  map(list.files, full.names = T) |> 
  map(filter_paths)
data_list <- map(file_paths, \(fp) data.frame(
  path = fp,
  season = str_extract(fp, "summer|winter"),
  number = as.integer(str_extract(fp, "(?<=/)(\\d+)(?=_)")),
  infected = str_detect(fp, "I")
) |> 
  mutate(season = factor(season, levels = c("winter", "summer"))) |> 
  arrange(number, season))
simulation_id <- results_dir[dc] |> str_sub(74) |> as.integer()

process_single_number <- function(df, number) {
  df %>%
    filter(number == !!number) %>%
    group_split(infected) %>%
    map(\(subset_df) pull(subset_df, path)) %>%
    map(\(l) map(l, qread)) |> 
    map(\(l) map(l, \(m) m[mg_trends$region_cell])) |> 
    map(\(df) Reduce(`+`, df)) |> 
    setNames(c("healthy", "sick")) |> 
    bind_cols() |> 
    mutate(index = number, region_cell = mg_trends$region_cell)
}

sick_healthy_sums <- function(df_list) {
  df_list |> 
    future_map(\(df) map_dfr(63:77, process_single_number, df = df)) |> 
    imap(\(df, i) mutate(df, sim = simulation_id[i])) |> 
    bind_rows()   
}

rmse <- function(predicted, observed) {
  sqrt(mean(observed - predicted)^2)
}

sick_healthy <- sick_healthy_sums(data_list)

prevalence_trend_metric <- function(df) {
  prevalence_metrics <- df |> 
  left_join(year_lookup, by = join_by(index)) %>%
  group_by(region_cell) |> 
  filter(sum(healthy) > 0 && sum(sick) > 0) |> 
  nest() |> 
  mutate(model = map(data, \(df) glm(cbind(sick, healthy) ~ Year, 
    family = binomial, data = df))) |> 
  mutate(Year_slope = map_dbl(model, \(m) coef(m)[2])) |> 
  left_join(mg_trends, by = join_by(region_cell)) |> 
  ungroup() |> 
  select(predicted = Year_slope, observed = estimate)

  lift(rmse)(prevalence_metrics)
}

prevalence_trend <- sick_healthy |> group_split(sim) |> future_map_dbl(prevalence_trend_metric)
hist(prevalence_trend)
```

This is wildly all over the place... probably not the best validation metric.

## Mycoplasma Presence

Here I take sites that have consistent presence of the disease over many years and see if the simulations can replicate this persistence.

```{r mg presence}
# Load the presence/absence data
pres_hfds <- read_csv(here("Data/Validation/mycoplasma_presence.csv"))

# Max penalty for simulations with no sick birds
max_penalty <- pres_hfds |> 
  rowwise() |> 
  mutate(range = length(min_index:max_index)) |> 
  pull(range) |> 
  sum()

# Extract the file paths for infected birds
infected_list <- results_dir[dc] |> 
  lapply(list.files, full.names = TRUE) |>  
  lapply(function(p) data.frame(
    path = p,
    index = as.numeric(str_split_i(p, "/", 10) |> str_split_i("_", 1)),
    infected = str_detect(p, "I")
  ) |> 
  filter(infected) |> 
  arrange(index) |> 
  left_join(year_lookup, by = join_by(index)) |> 
  filter(Year %in% 1994:2016))

no_infection <- sapply(infected_list, function(d) nrow(d) == 0)

# Pre-allocate the penalty vector
mg_presence_penalty <- rep(NA, length(infected_list))
mg_presence_penalty[no_infection] <- max_penalty

# Parallel processing with foreach
mg_presence_metric <- foreach(df = infected_list[!no_infection], .combine = c, .packages = c("dplyr", "purrr", "qs", "abind")) %dopar% {
  # Fill missing years and group by Year
  df_filled <- fill_missing_years(df) |> group_split(Year)
  
  # Process each year group
  sim_array <- map(df_filled, function(year_df) {
    # Load only the necessary files for this year's paths
    paths <- year_df |> pull(path)
    files <- lapply(paths, qread)  # Read files in this chunk
    combined_files <- lapply(files, function(f) Reduce(`+`, f))  # Combine the data
    
    # Create the array for this chunk
    array(combined_files, dim = c(106, 161, 1))
  }) |> abind()
  
  # Calculate the metric for this simulation
  pres_hfds |>
    rowwise() |>
    mutate(
      rowcol = list(convert_flat_to_2d(region_cell, 106)),
      sick = sum(sim_array[rowcol[1], rowcol[2], min_index:max_index]),
      none_sick = sick == 0
    ) |>
    pull(none_sick) |>
    sum()
}

# Assign metrics to the penalty vector
mg_presence_penalty[!no_infection] <- mg_presence_metric
hist(mg_presence_metric)
```

This is looking to be an excellent filter.

## Haemorhous Presence/Absence

Here I test the simulations at locations where, according to my observation data, the finches are either present for long periods of time or absent for long periods of time.

```{r presence absence}
# Load the presence/absence data
presabs <- here("Data/Validation/haemorhous_presence_absence.csv") |> 
  read_csv()

# Gather relevant data
presence_list <- results_dir[dc] |> 
  lapply(list.files, full.names = TRUE) |>  
  lapply(function(p) {
    data.frame(
      path = p,
      index = as.numeric(str_split_i(p, "/", 10) |> str_split_i("_", 1)),
      infected = str_detect(p, "I")
    ) |> 
    arrange(index) |> 
    left_join(year_lookup, by = join_by(index))
  })

# Parallel processing with foreach
presabs_metric <- foreach(plist = presence_list, .combine = c, .packages = c("dplyr", "purrr", "qs", "abind")) %dopar% {
  # Apply fill_missing_years and group by Year
  plist_filled <- plist |> fill_missing_years() |> group_split(Year)
  
  # Initialize an empty list to store results for this presence_list
  sim_array_results <- vector("list", length(plist_filled))
  
  # Process each grouped data frame separately to manage memory
  for (i in seq_along(plist_filled)) {
    df <- plist_filled[[i]]
    
    # Load only the necessary files for this chunk
    paths <- df$path
    files <- lapply(paths, qread)  # Read files in this chunk
    combined_files <- lapply(files, function(f) Reduce(`+`, f))  # Combine the data
    
    # Create the array for this chunk and store it
    sim_array_results[[i]] <- array(combined_files, dim = c(106, 161, 1))
  }
  
  # Combine all the arrays for the current presence_list
  sim_array <- abind(sim_array_results)
  
  # Calculate the metric for this presence_list
  penalty <- presabs |>
    rowwise() |>
    mutate(
      rowcol = list(convert_flat_to_2d(region_cell, 106)),
      occupancy = sum(sim_array[rowcol[1], rowcol[2], min_index:max_index] > 0),
      penalty = if_else(
        always_present,
        length(min_index:max_index) - occupancy,
        occupancy
      )
    ) |>
    pull(penalty) |>
    sum()
  
  return(penalty)
}
```

## Haemorhous Trends

I have information on trends in house finch abundance across North America. I just need to compare the trends in the simulations in the conservation regions of interest against the observed ones from Christmas Bird Count data.

```{r abundance trend}
# Read in trend data
trend1993 <- read_csv(here("Data/Validation/abundance_trend_1993on.csv"))
trend1970 <- read_csv(here("Data/Validation/abundance_trend_1970on.csv"))
# and conservation regions
bcr <- here("Data/Validation/bird_conservation_regions.qs") |> qread()

# Penalty function
abundance_trend_penalty <- function(percent_change, trend_upper, trend_lower) {
  if (any(is.na(c(trend_upper, trend_lower)))) {
    stop("The trend inputs must not be NA")
  }
  if (is.na(percent_change)) {
    penalty <- 20
  } else if (percent_change < trend_lower) {
    penalty <- trend_lower - percent_change
  } else if (percent_change > trend_upper) {
    penalty <- percent_change - trend_upper
  } else {
    penalty <- 0
  }
  return(penalty)
}

# Gather relevant data
abundance_list <- results_dir[dc] |> 
  lapply(list.files, full.names = TRUE) |>  
  lapply(function(p) {
    data.frame(
      path = p,
      index = as.numeric(str_split_i(p, "/", 10) |> str_split_i("_", 1)),
      infected = str_detect(p, "I")
    ) |> 
    arrange(index) |> 
    left_join(year_lookup, by = join_by(index)) |> 
    filter(Year %in% 1970:2016)
  })

trend_metric <- foreach(
  alist = abundance_list,
  .combine = c,
  .packages = c("dplyr", "purrr", "qs", "abind")
) %dopar% {
  
  # Preprocess data and fill missing years
  alist_filled <- alist |> fill_missing_years() |> group_split(Year)
  
  # Preallocate a 3D array for sim_array results
  sim_array_results <- array(0, dim = c(106, 161, length(alist_filled)))
  
  # Process each year chunk to manage memory and avoid redundant computations
  for (i in seq_along(alist_filled)) {
    df <- alist_filled[[i]]
    
    # Read files only once per chunk
    paths <- df$path
    files <- lapply(paths, qread)
    combined_files <- Reduce(`+`, files)  # Combine directly
    
    # Fill the preallocated array
    sim_array_results[, , i] <- combined_files
  }
  
  # Function to calculate trend metrics
  calculate_trend_metrics <- function(year_range, baseline_year) {
    abund <- map_dfr(5:37, function(int_val) {
      positions <- which(bcr == int_val, arr.ind = TRUE)
      subset_array <- sim_array[positions[, 1], positions[, 2], , drop = F]
      data.frame(
        Abundance = colSums(subset_array, dims = 2),
        bcr = rep(int_val),
        Year = c(1970:2016)
      )
    })
    
    trend_by_bcr <- abund |>
      filter(Year %in% year_range) |>
      group_by(bcr) |>
      nest() |>
      mutate(model = map(data, \(df) lm(Abundance ~ Year, data = df))) |>
      mutate(tidy = map(model, tidy)) |>
      unnest(tidy) |>
      filter(term == "Year")
    
    baseline <- map_dbl(trend_by_bcr$model, \(m) predict(m, newdata = data.frame(Year = baseline_year)))
    trend_by_bcr$percent_change <- (trend_by_bcr$estimate / baseline) * 100
    
    penalty <- trend_by_bcr |> 
      select(BCR = bcr, percent_change) |>
      right_join(if (baseline_year == 1970) trend1970 else trend1993) |>
      mutate(penalty = abundance_trend_penalty(percent_change, estimate_ucl, estimate_lcl)) |>
      pull(penalty) |>
      sum()
    
    return(penalty)
  }
  
  # Calculate penalties for both 1970 and 1993
  penalty1970 <- calculate_trend_metrics(1970:2016, 1970)
  penalty1993 <- calculate_trend_metrics(1993:2016, 1993)
  
  # Return the penalties
  c(penalty1993 = penalty1993, penalty1970 = penalty1970)
}

```

# Posteriors

First of all, I find it interesting to consider what filtering for presence of birds in DC does to the posteriors, since it's our first filter.

```{r}
round1_priors |> 
  mutate(dc = dc) |> 
  select(-mortality_Sa_summer, -mortality_Ra_summer) |> 
  pivot_longer(1:45, names_to = "Prior", values_to = "Value") |> 
  ggplot(aes(x = Value, group = dc, fill = dc)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~Prior, scales = "free") +
  scale_fill_paletteer_d("PrettyCols::Light", name = "Birds in DC?")
```

## 

# Top Models
