---
title: "mgsim: Round 3a Analysis"
author: "July Pilowsky"
format: html
editor: visual
toc: true
code-fold: true
lightbox: true
theme: materia
---

```{r setup, message = FALSE, echo = FALSE}
library(tidyplots)
library(paletteer)
library(here)
library(cowplot)
library(GGally)
library(abc)
library(tidyverse)
library(furrr)
library(bayestestR)
library(qs2)
plan(multicore)
source(here("Scripts/validation_metric_functions.R"))
source(here("Scripts/convenience_functions.R"))
source(here("Scripts/animate_sim.R"))
set_trust_promises(TRUE)
region <- qread(here("Data/Input/finch_region.qs"))
round2_metrics <- read_csv(here("Data/Validation/round2c_validation_metrics.csv"))
trend_metrics2 <- read_csv(here("Data/Validation/round2c_trend_metrics.csv")) |>
    pivot_longer(everything(), names_to = c("metric", "sim"), names_sep = "\\.\\.\\.")
round2_metrics$hm_trend_1970[round2_metrics$dc] <- trend_metrics2 |> filter(metric == "penalty1970") |> pull(value)
round2_metrics$hm_trend_1993[round2_metrics$dc] <- trend_metrics2 |> filter(metric == "penalty1993") |> pull(value)
round1_priors <- read_csv(here("Data/Input/sample_data_round1.csv"))
round1_metrics <- read_csv(here("Data/Validation/round1_validation_metrics.csv")) |>
    mutate(hm_trend_1970 = rep(NA_real_), hm_trend_1993 = rep(NA_real_))
round1_metrics$hm_trend_1970[round1_metrics$dc] <- round1_metrics$hm_trend[seq(1, 9752, 2)]
round1_metrics$hm_trend_1993[round1_metrics$dc] <- round1_metrics$hm_trend[seq(2, 9752, 2)]
round2_priors <- read_csv(here("Data/Input/sample_data_round2b.csv")) |>
  mutate(mortality_Sa_summer = round1_priors$mortality_Sa_summer,
         mortality_Ra_summer = mortality_Sa_summer)
round3a_priors <- read_csv(here("Data/Input/sample_data_round3a.csv")) |>
  mutate(mortality_Sa_summer = round1_priors$mortality_Sa_summer,
         mortality_Ra_summer = mortality_Sa_summer)
round3a_metrics <- read_csv(here("Data/Validation/round3a_validation_metrics.csv"))
trend_metrics3a <- read_csv(here("Data/Validation/round3a_trend_metrics.csv")) |>
    pivot_longer(everything(), names_to = c("metric", "sim"), names_sep = "\\.\\.\\.")
round3a_metrics$hm_trend_1970[round3a_metrics$dc] <- trend_metrics3a |> filter(metric == "penalty1970") |> pull(value)
round3a_metrics$hm_trend_1993[round3a_metrics$dc] <- trend_metrics3a |> filter(metric == "penalty1993") |> pull(value)
```

# What is my goal in Round 3a of simulation?

My goal is to improve over Rounds 1 and 2 on the validation metrics, and to determine whether the posteriors have stabilized and I can stop simulating.

# Validation Metric Performance

## Presence in DC

To start with, let's look at presence of finches in DC. In the first round, 4876 simulations out of 10000 simulated the presence of house finches in Washington, DC in 1994, and in Round 2, it was 6992 out of 10000.

```{r presence_dc}
sum(round3a_metrics$dc)
```

3,140 simulations have birds present in DC, which is worse than the two previous rounds. Ouch. Let me examine again which parameter settings make simulations pass this filter or not:

```{r dc}
round3a_priors |>
  mutate(dc = round3a_metrics$dc) |>
  select(-mortality_Sa_summer, -mortality_Ra_summer) |>
  pivot_longer(1:45, names_to = "Prior", values_to = "Value") |>
  ggplot(aes(x = Value, group = dc, fill = dc)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~Prior, scales = "free") +
  scale_fill_paletteer_d("PrettyCols::Light", name = "Birds in DC?")
```

The controls on what passes the DC filter or not are different from Round 1. In Round 1, it had to do entirely with dispersal_target_n_k, maximizing the range of population densities that the finches were willing to migrate to. In this round, that's already been optimized, so instead we're selecting for low quasi-extinction threshold, higher fecundity, intermediate juvenile dispersal distance, and widening dispersal_source_n_k (the range of population densities that will push finches out and toward dispersal.)

## Other Validation Metrics

Moving on I can look at the other validation metrics:

```{r validation_metrics_plot, warning = FALSE}
validation_metrics <- bind_rows(round1_metrics, round2_metrics, round3a_metrics) |>
                      mutate(Round = factor(c(rep("1", 10000), rep("2", 10000), rep("3a", 10000)))) |>
                      filter(dc) |>
                      select(-hm_trend, -index, -dc)
quantiles <- validation_metrics[,1:7] |>
  map_dbl(\(x) quantile(x, prob = c(0.001), na.rm = T))
labels <- c("Mycoplasma presence", "Finch pres/abs",
            "Point prevalence", "Mycoplasma arrival", "Finch arrival",
            "Finch trend from 1970", "Finch trend from 1993")
plotlist <- pmap(list(names(validation_metrics)[1:7], quantiles, labels), function(x, y, z) {
    outliers_removed <- validation_metrics[, c(x, "Round")][pull(validation_metrics, x) < quantile(pull(validation_metrics, x), 0.95, na.rm=T), ]
    ggplot(data = outliers_removed, aes(x = .data[[x]], fill = Round, group = Round)) +
    geom_histogram(position = "dodge") +
    geom_vline(xintercept = y, color = "blue") +
    xlab(z) +
    scale_fill_paletteer_d("PNWColors::Sunset2")
})
plot_grid(plotlist = plotlist)
```

Performance has improved on the metrics used for ABC (*Mycoplasma* presence and finch first arrival) and also on metrics that were not used for validation (finch presence/absence, finch trend from 1993.)

# Selected models

I will continue with the same two metrics used in the previous rounds, for consistency.

```{r abc_two_metrics}
abc31 <- abc(param = round3a_priors[round3a_metrics$dc,],
             sumstat = round3a_metrics |> filter(dc) |> select(c(3,7)),
              target = rep(0, 2),
             tol = 0.02,
             method = "neuralnet")
qsave(abc31, here("Data/Validation/abc_round3a.qs"))
head(abc31$ss, 10)
selected_samples_3 <- round3a_priors |>
  rowid_to_column("sample") |>
  semi_join(abc31$unadj.values, copy = TRUE) |>
  pull(sample)
write_sftp_transfer("/glade/work/pilowskyj/Round3a",
                      "/Users/julypilowsky/Documents/mgsim/Data/Output/Round3a",
                      selected_samples_3,
                      here("Scripts/Globus/abc_round3a.txt"))
ensemble_sa <- ensemble_mean(selected_samples_3, abc31$weights, "Sa", here("Data/Output/Round3a"))
animate_sim(ensemble_sa, region, remove_outliers = TRUE)
```

This simulation doesn't look bad, though I definitely need to talk with house finch biologists about the realism of the pattern of finch spread on the landscape.

```{r animate_recovered_adults}
ensemble_ra <- ensemble_mean(selected_samples_3, abc31$weights, "Ra", here("Data/Output/Round3a"))
animate_sim(ensemble_ra, region, years = 1994:2016, burn_in = 111)
```

The disease dynamics look really cool here, though!

## Posterior Distributions

```{r optimized abc}
plot_abc_posteriors(abc31, round2_priors)
```

# Is it time to stop modeling?

We use two checks to see if it's time to stop modeling: Bayes factors and posterior predictive checks.

```{r bayes_factor}
bayesfactor_parameters(posterior = as.data.frame(abc31$adj.values)[1:45], 
                       prior = round3a_priors[1:45])
```

Bayes factors look good! They are all well below 1.

The final confirmation is to do posterior predictive checks. The way I do this is to do a small batch of pilot simulations based on the posteriors of round 3, run validation metrics, and make sure that the validation targets fall neatly within the distribution of observed validation metrics.